{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import rankdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('article_user.csv')\n",
    "articles = pd.read_csv('articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(users.user_id.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11k users have read 20 articles, The rest of the users \n",
    "We may have good predcitions for the 20artilce users users but not for the 3article users users with the simple content based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram((users.timestamp - users.timestamp.min())/(60*60*24), title='Time histogram resampled to hourly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No data gaps in the timeline of the data - Can use all data as training set without concerns\n",
    "\n",
    "N of articles has a periodic daily behavior. Probably users read less news at night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicated data in users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Nans in the data and duplicates dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(users.article_id.unique()) == list(articles.article_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a headline for every article in the users table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(pd.to_datetime(articles['published_date'], format='%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most users are from 2021, in first approximation I can recommend any article on this set\n",
    "\n",
    "Extremely likely the recommended article will be a new article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Content based recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only feature I have to describe an article is the headline \n",
    "<br>\n",
    "I can suggest an articles that is similar to the articles the user have read in the past\n",
    "<br>\n",
    "I need a metric to indicate how similar 2 articles are: I will use a vector representation for the headline and a similarity metric for two vectors\n",
    "<br>\n",
    "The 2 vector approaches to explore are TfIdf and Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = \"http://proxy-chain.intel.com:911\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['http_proxy'] = proxy \n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of TfIdf and Bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender():\n",
    "    def __init__(self, articles):\n",
    "        self.articles = articles\n",
    "        self.cosine_sim = None\n",
    "        self.top5 = None\n",
    "    def train_tfidf(self, train):\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform(train.headline)\n",
    "        self.cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    def train_bert(self, train):\n",
    "        model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "        sentence_embeddings = model.encode(train.headline)\n",
    "        self.cosine_sim =  cosine_similarity(sentence_embeddings,sentence_embeddings)\n",
    "    def load_top5_articles(self, users):\n",
    "        top5_id= users.article_id.value_counts()[0:5]\n",
    "        top5_art =pd.merge(top5_id,self.articles,left_index=True,right_on ='article_id')\n",
    "        self.top5 = top5_art['headline']\n",
    "    def recommend_top5(self, user):\n",
    "        \"\"\"Alwasys recommends the top5 articles across all data\"\"\"\n",
    "        return self.top5\n",
    "    def recommend(self, user ,users):\n",
    "        \"\"\"Recommend articles for user\"\"\"\n",
    "        prev_articles = self.prev_articlesid(user,users)\n",
    "        similar = self.similar5( prev_articles)\n",
    "        return self.id2headline(similar)\n",
    "    def prev_articlesid(self, user,users):\n",
    "        \"\"\"all previous articles for user in users df\"\"\"\n",
    "        return  users[users.user_id==user]['article_id']\n",
    "    def similar_vector(self, prev_articles):\n",
    "        \"\"\"vector of lenght number of headlines representing how similar a headine is to previous articles\"\"\"\n",
    "        idxs = self.articles[self.articles['article_id'].isin(prev_articles)  ].index\n",
    "        vectors = self.cosine_sim[idxs]\n",
    "        vector = vectors.mean(axis=0) # mean across all previous articles, in a next level model most recent articles can have more weight\n",
    "        return vector\n",
    "    def similar5(self,prev_articles ):\n",
    "        vector = self.similar_vector(prev_articles)\n",
    "        ranks = len(self.articles)- rankdata(vector) # inverting  so rank 0 is the first recommendation\n",
    "        idxs = ranks.argsort()[:5]\n",
    "        return articles.iloc[idxs]['article_id']\n",
    "        \n",
    "    def id2headline(self, article_ids):\n",
    "        return self.articles[self.articles['article_id'].isin(article_ids)]['headline']\n",
    "    def rank_article(self, user,users,  articleid):\n",
    "        \"\"\"Returns the rank in the recommendation system for the article\"\"\"\n",
    "        prev_articles = self.prev_articlesid( user,users)\n",
    "        vector = self.similar_vector( prev_articles)\n",
    "        ranks = len(self.articles)- rankdata(vector) # inverting  so rank 0 is the first recommendation\n",
    "        idx = self.articles[self.articles.article_id == articleid ].index[0]\n",
    "        return ranks[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class recommender provides methods for recommendations using two models, they both use vector representations for the headlines \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the users dataset into test and train\n",
    "Test is the composed of the last article for every user and train is the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = users.loc[users.groupby(\"user_id\")[\"timestamp\"].idxmin()]\n",
    "train = pd.concat([users, test]).drop_duplicates(keep=False)\n",
    "test.reset_index(drop=True,inplace=True)\n",
    "train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train) +len(test) , len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning the system with the headlines and the tfidf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = Recommender(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "recommender.train_tfidf(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.cosine_sim.shape, len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the similarity matrix for N of headlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = test.user_id.iloc[0]\n",
    "user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender method that suggest top 5 articles for a given user with the tdidf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.recommend(user,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender method that suggest top 5 articles for a given user with the bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "recommender.train_bert(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.recommend(user,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender method that suggest top 5 most popular articles regardless of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.load_top5_articles(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.recommend_top5(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  reasonable approach is to use the naive top 5 most popular articles when there is no previous data for the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tfidf revision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TfIdf if t is the term and h is the headline, then:\n",
    "    \n",
    "tf(t,h) = count of t in d / number of words in h\n",
    "<br>\n",
    "df(t) = occurrence of t in all headlines\n",
    "<br>\n",
    "idf(t) = log(N/ df(t))\n",
    "\n",
    "And Tfidf is defined as:\n",
    "\n",
    "tf-idf(t, d) = tf(t, d) * idf(t)\n",
    "\n",
    "*So TfIdf is high for terms that are frequent in the headline and is penalized if the term is common across multiple headlines*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of TfidfVectorizer with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_matrix = tfidf.fit_transform(articles.headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirming the TfidfVectorizer matrix is consistent with our expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the matrix is len(articles) by Number of terms across all headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.DataFrame(tfidf.idf_, index=tfidf.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "df_idf.sort_values(by=['idf_weights'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idf part of Tfidf is working as expected\n",
    "<br>\n",
    "Kremlin, monument are very unique words across all headlines, while trump, biden, ... are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse matrix to dense, adding the terms back and pringing not zeros\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.todense()[0].T, index=tfidf.get_feature_names(),columns=[\"tfidf\"]) \n",
    "df_tfidf[df_tfidf.tfidf != 0].sort_values(by='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf is working as expected for the first headline\n",
    "<br>\n",
    "\"on, as, it, etc\" are scrapped because of the *stop_words='english'* option used to build the matrix\n",
    "<br>\n",
    "There are not repeating words in the tittle then Tfidf is dominated by idf which is low for \"Biden\" (common word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Similarity Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the similarity metric we can use the cosine function\n",
    "<br>\n",
    "Each headline is represented by a vector in our tfidf matrix and the similarity between two healines will be the cosine between the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine_sim shape is Nheadlines x Nheadlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bert representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT stands for Bidirectional Encoder Representations from Transformers it is a pre-train model arquitecture developed on 2018 (https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "We used the implemenation in the SentenceTransformers libray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: The recomendation model is user driven, so I have to  evaluate on every user: \n",
    "\n",
    "**test set**: the last article read by every user  \n",
    "\n",
    "**train set**: The rest of the dataset, the data provides at least 3 articles for every user so train will have at least 2 articles per user\n",
    "\n",
    "\n",
    "**metric** I will calculate Ranking for the test article - A number from 0 to total number of articles (1455) indicating how the model will rank this article. 0 being the first article to recommend and 1455 the last article to recommend\n",
    "\n",
    "** Potential improvements to the evaulation:**\n",
    "\n",
    "Crossvalidation test will give more reliable results\n",
    "\n",
    "Deploying to prod the model and AB test would be the best possible evalution of the model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Ranks for the TfIdf  model for the users in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "recommender.train_tfidf(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bert_ranks = test.apply(lambda x: recommender.rank_article( x.user_id, train,  x.article_id) , axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Ranks for the Bert model for the users in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "recommender.train_tfidf(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_ranks = test.apply(lambda x: recommender.rank_article( x.user_id, train,  x.article_id) , axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the ranks takes ~20 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_ranks.to_csv('bert.csv')\n",
    "tfidf_ranks.to_csv('tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the ranks to avoid recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ranks = test.copy()\n",
    "test_ranks['NarticlesinTrain'] =  train.groupby(['user_id'])['article_id'].count().reset_index()['article_id']\n",
    "test_ranks['bert'] = len(articles) - bert_ranks['0']\n",
    "test_ranks['tfidf'] = len(articles) - tfidf_ranks['0']\n",
    "test_ranks['NarticlesinTrain'] = test_ranks.NarticlesinTrain.astype('float')\n",
    "test_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ranks['articlesTrain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=test_ranks['bert'],name='bert'))\n",
    "fig.add_trace(go.Histogram(x=test_ranks['tfidf'],name = 'tfidf'))\n",
    "fig.update_layout(title = 'Histograms of the Ranks given for the test article for both models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph each bar represents the number of times that an article got ranked at that value\n",
    "\n",
    "The best model will rank most of these articles near zero. A few observations:\n",
    "    1. The TfIdf model is better sugessting the top 10 articles. \n",
    "    2. The Bert model is better in the 10-700 range\n",
    "    3. Bert is monotonically decreasing to lower ranks, this what is expected for the model as we want the model to give high number of good predictions and low number of bad ones\n",
    "    4. TfIdf is not monotonic with some more peak at around 800 -1000, this would indicate the model may not generalize well to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(test_ranks.sort_values(['NarticlesinTrain']),x='bert',facet_col='NarticlesinTrain', facet_col_wrap=5,title =\"Histogram of ranks per N of articles in the train set for the Bert model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(test_ranks.sort_values(['NarticlesinTrain']),x='tfidf',facet_col='NarticlesinTrain', facet_col_wrap=5,title =\"Histogram of ranks per N of articles in the train set for the tfidf model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous plots we partitionate the ranking hisotograms by the Number of articles in the train data for that user:\n",
    "    1. The 800-1000 unexpected bump in the tfidf model is coming from the users with very low data NarticlesTrain = 2 or 3, this indicates that tfidf does a bad job with very limited data. Bert does not have that issue, even with very low data (2,3,4) the histogram is sill monotonic\n",
    "    2. For Bert the histogram seem to increase in slop with larger NarticlesinTrain, meaning that the prediction power does benefit from more data, which is not the case for TfIdf\n",
    "    3. TfIdf shows the 0-10 range peak regardless of the NarticlesinTrain\n",
    "    \n",
    "**Bert is probably going to generalize better in both extremes less data and more data pre user**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that high ranking values are due to predicting same read articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Isrecommendedseen(user, train, articleid):\n",
    "    prev = recommender.prev_articlesid( user, train)\n",
    "    return articleid in prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Isrecommendedseen(users_top5_bert.user_id.iloc[0],train, users_top5_bert.article_id.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "recommender.train_bertertert(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "users_top5_bert = user_Narticles[user_Narticles.bert <6]\n",
    "users_top5_seen_bert = users_top5_bert.apply(lambda x: Isrecommendedseen(x.user_id,train,x.article_id) , axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_top5_seen_bert.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "recommender.train_tfidf(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "users_top5_tfidf = user_Narticles[user_Narticles.tfidf <6]\n",
    "users_top5_seen_tfidf = users_top5_tfidf.apply(lambda x: Isrecommendedseen(x.user_id,train,x.article_id) , axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_top5_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Isrecommendedseen(users_top5_tfidf.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_top5_seen_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users_top5_bert) , len(users_top5_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## peak at 800 rank for tfidf          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_Narticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(user_Narticles,x='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(user_Narticles,x='bert',color_discrete_map=='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ranks_inv = len(articles)-  tfidf_ranks\n",
    "bert_ranks_inv = len(articles)-  bert_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ranks.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ranks.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert is monotonic while tfidf has a second/rhird peak behavior around 700/550\n",
    "- The additional peaks maybe caused by matching of words that have no strong significance\n",
    "\n",
    "Bert predicts more often high ranks (above 200 ranks) However tfidf predicts better for very high ranks 0-15\n",
    "This maybe possible for overfitting words very specific to this dataset (Trump,Biden, covid)? \n",
    "This may not generalize well to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ranks_inv.value_counts().sort_values().iloc[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ranks_inv.value_counts().sort_values().iloc[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ranks_inv.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "#fig.add_trace(go.Histogram(x=bert_ranks,name='bert'))\n",
    "fig.add_trace(go.Histogram(x=tfidf_ranks,name = 'tfidf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ranks.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ranks.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing both prediction modesl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model vs Number of articles available per reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next improvements to validity metric:\n",
    "\n",
    "    \n",
    "\n",
    "Cross validation \n",
    "The best approach for this would be AB Testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
